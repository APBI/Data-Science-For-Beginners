<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58860ce9a4b8a564003d2752f7c72851",
  "translation_date": "2025-10-03T15:59:55+00:00",
  "source_file": "1-Introduction/02-ethics/README.md",
  "language_code": "de"
}
-->
# Einf√ºhrung in Datenethik

|![ Sketchnote von [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| Datenethik - _Sketchnote von [@nitya](https://twitter.com/nitya)_ |

---

Wir sind alle Datenb√ºrger, die in einer datengetriebenen Welt leben.

Markttrends zeigen, dass bis 2022 jede dritte gro√üe Organisation ihre Daten √ºber Online-[Marktpl√§tze und B√∂rsen](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/) kaufen und verkaufen wird. Als **App-Entwickler** wird es f√ºr uns einfacher und kosteng√ºnstiger, datengesteuerte Erkenntnisse und algorithmusgesteuerte Automatisierung in den t√§glichen Nutzererfahrungen zu integrieren. Doch w√§hrend KI allgegenw√§rtig wird, m√ºssen wir auch die potenziellen Sch√§den verstehen, die durch die [Instrumentalisierung](https://www.youtube.com/watch?v=TQHs8SA1qpk) solcher Algorithmen in gro√üem Ma√üstab entstehen k√∂nnen.

Trends deuten darauf hin, dass wir bis 2025 √ºber [180 Zettabyte](https://www.statista.com/statistics/871513/worldwide-data-created/) an Daten erzeugen und konsumieren werden. F√ºr **Datenwissenschaftler** bietet diese Informationsflut beispiellosen Zugang zu pers√∂nlichen und Verhaltensdaten. Damit kommt die Macht, detaillierte Nutzerprofile zu erstellen und subtil Entscheidungen zu beeinflussen ‚Äì oft auf eine Weise, die eine [Illusion der freien Wahl](https://www.datasciencecentral.com/the-pareto-set-and-the-paradox-of-choice/) f√∂rdert. Dies kann genutzt werden, um Nutzer zu bevorzugten Ergebnissen zu lenken, wirft jedoch auch kritische Fragen zu Datenschutz, Autonomie und den ethischen Grenzen algorithmischer Einflussnahme auf.

Datenethik sind heute _notwendige Leitplanken_ f√ºr Datenwissenschaft und Ingenieurwesen, um potenzielle Sch√§den und unbeabsichtigte Konsequenzen unserer datengetriebenen Handlungen zu minimieren. Der [Gartner Hype Cycle f√ºr KI](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) identifiziert relevante Trends in digitaler Ethik, verantwortungsvoller KI und KI-Governance als Schl√ºsseltreiber f√ºr gr√∂√üere Megatrends rund um die _Demokratisierung_ und _Industrialisierung_ von KI.

![Gartner's Hype Cycle f√ºr KI - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

In dieser Lektion werden wir den faszinierenden Bereich der Datenethik erkunden ‚Äì von grundlegenden Konzepten und Herausforderungen bis hin zu Fallstudien und angewandten KI-Konzepten wie Governance ‚Äì, die dazu beitragen, eine Ethikkultur in Teams und Organisationen zu etablieren, die mit Daten und KI arbeiten.

## [Quiz vor der Vorlesung](https://ff-quizzes.netlify.app/en/ds/quiz/2) üéØ

## Grundlegende Definitionen

Beginnen wir mit dem Verst√§ndnis der grundlegenden Terminologie.

Das Wort "Ethik" stammt aus dem [griechischen Wort "ethikos"](https://en.wikipedia.org/wiki/Ethics) (und dessen Wurzel "ethos"), was _Charakter oder moralische Natur_ bedeutet.

**Ethik** bezieht sich auf die gemeinsamen Werte und moralischen Prinzipien, die unser Verhalten in der Gesellschaft bestimmen. Ethik basiert nicht auf Gesetzen, sondern auf allgemein akzeptierten Normen dessen, was "richtig vs. falsch" ist. Dennoch k√∂nnen ethische √úberlegungen Initiativen zur Unternehmensf√ºhrung und staatliche Vorschriften beeinflussen, die mehr Anreize f√ºr die Einhaltung schaffen.

**Datenethik** ist ein [neuer Zweig der Ethik](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1), der "moralische Probleme im Zusammenhang mit _Daten, Algorithmen und entsprechenden Praktiken_ untersucht und bewertet". Hierbei konzentriert sich **"Daten"** auf Aktionen im Zusammenhang mit Erzeugung, Aufzeichnung, Pflege, Verarbeitung, Verbreitung, Teilen und Nutzung, **"Algorithmen"** auf KI, Agenten, maschinelles Lernen und Roboter, und **"Praktiken"** auf Themen wie verantwortungsvolle Innovation, Programmierung, Hacking und Ethikkodizes.

**Angewandte Ethik** ist die [praktische Anwendung moralischer √úberlegungen](https://en.wikipedia.org/wiki/Applied_ethics). Es handelt sich um den Prozess der aktiven Untersuchung ethischer Fragen im Kontext von _realen Handlungen, Produkten und Prozessen_ und der Ergreifung von Korrekturma√ünahmen, um sicherzustellen, dass diese mit unseren definierten ethischen Werten √ºbereinstimmen.

**Ethikkultur** bezieht sich auf die [_Operationalisierung_ der angewandten Ethik](https://hbr.org/2019/05/how-to-design-an-ethical-organization), um sicherzustellen, dass unsere ethischen Prinzipien und Praktiken konsistent und skalierbar in der gesamten Organisation √ºbernommen werden. Erfolgreiche Ethikkulturen definieren organisationsweite ethische Prinzipien, bieten sinnvolle Anreize f√ºr die Einhaltung und verst√§rken ethische Normen, indem sie gew√ºnschte Verhaltensweisen auf allen Ebenen der Organisation f√∂rdern und verst√§rken.

## Ethikkonzepte

In diesem Abschnitt werden wir Konzepte wie **gemeinsame Werte** (Prinzipien) und **ethische Herausforderungen** (Probleme) f√ºr die Datenethik diskutieren ‚Äì und **Fallstudien** untersuchen, die Ihnen helfen, diese Konzepte in realen Kontexten zu verstehen.

### 1. Ethische Prinzipien

Jede Datenethikstrategie beginnt mit der Definition von _ethischen Prinzipien_ ‚Äì den "gemeinsamen Werten", die akzeptables Verhalten beschreiben und konforme Handlungen in unseren Daten- und KI-Projekten leiten. Sie k√∂nnen diese auf individueller oder Teamebene definieren. Die meisten gro√üen Organisationen legen diese jedoch in einer _ethischen KI-Missionserkl√§rung_ oder einem Rahmenwerk fest, das auf Unternehmensebene definiert und konsistent √ºber alle Teams hinweg durchgesetzt wird.

**Beispiel:** Microsofts [Verantwortungsvolle KI](https://www.microsoft.com/en-us/ai/responsible-ai)-Missionserkl√§rung lautet: _"Wir sind der F√∂rderung von KI verpflichtet, die von ethischen Prinzipien geleitet wird und den Menschen in den Mittelpunkt stellt"_ ‚Äì und identifiziert 6 ethische Prinzipien im untenstehenden Rahmenwerk:

![Verantwortungsvolle KI bei Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

Lassen Sie uns diese Prinzipien kurz erkunden. _Transparenz_ und _Verantwortlichkeit_ sind grundlegende Werte, auf denen andere Prinzipien aufbauen ‚Äì beginnen wir also damit:

* [**Verantwortlichkeit**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) macht Praktiker _verantwortlich_ f√ºr ihre Daten- und KI-Operationen sowie die Einhaltung dieser ethischen Prinzipien.
* [**Transparenz**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) stellt sicher, dass Daten- und KI-Aktionen f√ºr Nutzer _verst√§ndlich_ (interpretierbar) sind und die Entscheidungen sowie deren Gr√ºnde erkl√§rt werden.
* [**Fairness**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) ‚Äì konzentriert sich darauf, sicherzustellen, dass KI _alle Menschen_ fair behandelt und systemische oder implizite sozio-technische Vorurteile in Daten und Systemen angeht.
* [**Zuverl√§ssigkeit & Sicherheit**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) ‚Äì stellt sicher, dass KI _konsistent_ mit definierten Werten agiert und potenzielle Sch√§den oder unbeabsichtigte Konsequenzen minimiert.
* [**Privatsph√§re & Sicherheit**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) ‚Äì bezieht sich auf das Verst√§ndnis der Datenherkunft und bietet Nutzern _Datenschutz und verwandte Schutzma√ünahmen_.
* [**Inklusivit√§t**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) ‚Äì geht darum, KI-L√∂sungen mit Absicht zu gestalten und sie so anzupassen, dass sie _eine breite Palette menschlicher Bed√ºrfnisse_ und F√§higkeiten erf√ºllen.

> üö® √úberlegen Sie, was Ihre Datenethik-Missionserkl√§rung sein k√∂nnte. Erkunden Sie ethische KI-Rahmenwerke anderer Organisationen ‚Äì hier sind Beispiele von [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles) und [Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). Welche gemeinsamen Werte haben sie? Wie beziehen sich diese Prinzipien auf das KI-Produkt oder die Branche, in der sie t√§tig sind?

### 2. Ethische Herausforderungen

Sobald wir ethische Prinzipien definiert haben, besteht der n√§chste Schritt darin, unsere Daten- und KI-Aktionen zu bewerten, um festzustellen, ob sie mit diesen gemeinsamen Werten √ºbereinstimmen. Denken Sie √ºber Ihre Aktionen in zwei Kategorien nach: _Datenerhebung_ und _Algorithmendesign_.

Bei der Datenerhebung werden Aktionen wahrscheinlich **pers√∂nliche Daten** oder pers√∂nlich identifizierbare Informationen (PII) f√ºr identifizierbare lebende Personen umfassen. Dazu geh√∂ren [verschiedene Arten nicht-personenbezogener Daten](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en), die _zusammen_ eine Person identifizieren k√∂nnen. Ethische Herausforderungen k√∂nnen sich auf _Datenschutz_, _Datenbesitz_ und verwandte Themen wie _informierte Zustimmung_ und _geistige Eigentumsrechte_ f√ºr Nutzer beziehen.

Beim Algorithmendesign umfassen Aktionen das Sammeln und Pflegen von **Datens√§tzen**, die dann verwendet werden, um **Datenmodelle** zu trainieren und einzusetzen, die Ergebnisse vorhersagen oder Entscheidungen in realen Kontexten automatisieren. Ethische Herausforderungen k√∂nnen sich aus _Datensatzverzerrungen_, _Datenqualit√§tsproblemen_, _Unfairness_ und _Fehldarstellungen_ in Algorithmen ergeben ‚Äì einschlie√ülich einiger systemischer Probleme.

In beiden F√§llen heben ethische Herausforderungen Bereiche hervor, in denen unsere Aktionen m√∂glicherweise mit unseren gemeinsamen Werten in Konflikt geraten. Um diese Bedenken zu erkennen, zu mindern, zu minimieren oder zu beseitigen, m√ºssen wir moralische "Ja/Nein"-Fragen zu unseren Aktionen stellen und gegebenenfalls Korrekturma√ünahmen ergreifen. Schauen wir uns einige ethische Herausforderungen und die moralischen Fragen an, die sie aufwerfen:

#### 2.1 Datenbesitz

Die Datenerhebung umfasst oft pers√∂nliche Daten, die die Datensubjekte identifizieren k√∂nnen. [Datenbesitz](https://permission.io/blog/data-ownership) bezieht sich auf _Kontrolle_ und [_Nutzerrechte_](https://permission.io/blog/data-ownership) im Zusammenhang mit der Erstellung, Verarbeitung und Verbreitung von Daten.

Die moralischen Fragen, die wir stellen m√ºssen, sind:
 * Wem geh√∂ren die Daten? (Nutzer oder Organisation)
 * Welche Rechte haben Datensubjekte? (z. B. Zugriff, L√∂schung, √úbertragbarkeit)
 * Welche Rechte haben Organisationen? (z. B. Korrektur b√∂swilliger Nutzerbewertungen)

#### 2.2 Informierte Zustimmung

[Informierte Zustimmung](https://legaldictionary.net/informed-consent/) definiert den Akt, bei dem Nutzer einer Aktion (wie der Datenerhebung) mit einem _vollst√§ndigen Verst√§ndnis_ relevanter Fakten einschlie√ülich Zweck, potenzieller Risiken und Alternativen zustimmen.

Fragen, die hier zu erkunden sind:
 * Hat der Nutzer (Datensubjekt) die Erfassung und Nutzung von Daten erlaubt?
 * Hat der Nutzer den Zweck verstanden, f√ºr den die Daten erfasst wurden?
 * Hat der Nutzer die potenziellen Risiken seiner Teilnahme verstanden?

#### 2.3 Geistiges Eigentum

[Geistiges Eigentum](https://en.wikipedia.org/wiki/Intellectual_property) bezieht sich auf immaterielle Sch√∂pfungen, die aus menschlicher Initiative resultieren und _wirtschaftlichen Wert_ f√ºr Einzelpersonen oder Unternehmen haben k√∂nnen.

Fragen, die hier zu erkunden sind:
 * Hatten die gesammelten Daten wirtschaftlichen Wert f√ºr einen Nutzer oder ein Unternehmen?
 * Hat der **Nutzer** hier geistiges Eigentum?
 * Hat die **Organisation** hier geistiges Eigentum?
 * Wenn diese Rechte existieren, wie sch√ºtzen wir sie?

#### 2.4 Datenschutz

[Datenschutz](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) oder Informationsschutz bezieht sich auf die Wahrung der Privatsph√§re und den Schutz der Identit√§t von Nutzern in Bezug auf pers√∂nlich identifizierbare Informationen.

Fragen, die hier zu erkunden sind:
 * Sind die (pers√∂nlichen) Daten der Nutzer gegen Hacks und Lecks gesichert?
 * Sind die Daten der Nutzer nur f√ºr autorisierte Nutzer und Kontexte zug√§nglich?
 * Wird die Anonymit√§t der Nutzer gewahrt, wenn Daten geteilt oder verbreitet werden?
 * Kann ein Nutzer aus anonymisierten Datens√§tzen de-identifiziert werden?

#### 2.5 Recht auf Vergessenwerden

Das [Recht auf Vergessenwerden](https://en.wikipedia.org/wiki/Right_to_be_forgotten) oder [Recht auf L√∂schung](https://www.gdpreu.org/right-to-be-forgotten/) bietet Nutzern zus√§tzlichen Schutz pers√∂nlicher Daten. Insbesondere gibt es Nutzern das Recht, die L√∂schung oder Entfernung pers√∂nlicher Daten aus Internetsuchen und anderen Orten _unter bestimmten Umst√§nden_ zu verlangen ‚Äì und ihnen so einen Neuanfang online zu erm√∂glichen, ohne dass vergangene Handlungen gegen sie verwendet werden.

Fragen, die hier zu erkunden sind:
 * Erm√∂glicht das System Datensubjekten, die L√∂schung zu beantragen?
 * Sollte der Widerruf der Nutzerzustimmung eine automatisierte L√∂schung ausl√∂sen?
 * Wurden Daten ohne Zustimmung oder auf rechtswidrige Weise erhoben?
 * Sind wir konform mit staatlichen Vorschriften zum Datenschutz?

#### 2.6 Datensatzverzerrung

Datensatz- oder [Erhebungsverzerrung](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) bezieht sich auf die Auswahl eines _nicht repr√§sentativen_ Teils von Daten f√ºr die Algorithmusentwicklung, was potenzielle Unfairness in den Ergebnissen f√ºr verschiedene Gruppen schaffen kann. Arten von Verzerrungen umfassen Auswahl- oder Stichprobenverzerrung, Freiwilligenverzerrung und Instrumentenverzerrung.

Fragen, die hier zu erkunden sind:
 * Haben wir eine repr√§sentative Gruppe von Datensubjekten rekrutiert?
 * Haben wir unseren gesammelten oder gepflegten Datensatz auf verschiedene Verzerrungen getestet?
 * K√∂nnen wir entdeckte Verzerrungen mindern oder entfernen?

#### 2.7 Datenqualit√§t

[Datenqualit√§t](https://lakefs.io/data-quality-testing/) untersucht die Validit√§t des gepflegten Datensatzes, der zur Entwicklung unserer Algorithmen verwendet wird, und pr√ºft, ob Merkmale und Datens√§tze die Anforderungen an die Genauigkeit und Konsistenz f√ºr unseren KI-Zweck erf√ºllen.

Fragen, die hier zu erkunden sind:
 * Haben wir g√ºltige _Merkmale_ f√ºr unseren Anwendungsfall erfasst?
 * Wurden Daten _konsistent_ √ºber verschiedene Datenquellen hinweg erfasst?
 * Ist der Datensatz _vollst√§ndig_ f√ºr verschiedene Bedingungen oder Szenarien?
* Wird die Information _genau_ erfasst und spiegelt sie die Realit√§t wider?

#### 2.8 Fairness von Algorithmen

[Fairness von Algorithmen](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) untersucht, ob das Design des Algorithmus systematisch bestimmte Untergruppen von Datenpersonen diskriminiert, was zu [potenziellen Sch√§den](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) in der _Ressourcenverteilung_ (bei der Ressourcen verweigert oder zur√ºckgehalten werden) und der _Servicequalit√§t_ (bei der KI f√ºr einige Untergruppen weniger genau ist als f√ºr andere) f√ºhren kann.

Fragen, die hier untersucht werden sollten:
* Haben wir die Modellgenauigkeit f√ºr verschiedene Untergruppen und Bedingungen bewertet?
* Haben wir das System auf potenzielle Sch√§den (z. B. Stereotypisierung) √ºberpr√ºft?
* K√∂nnen wir Daten √ºberarbeiten oder Modelle neu trainieren, um identifizierte Sch√§den zu mindern?

Erkunden Sie Ressourcen wie [Checklisten zur Fairness von KI](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA), um mehr zu erfahren.

#### 2.9 Fehlrepr√§sentation

[Fehlrepr√§sentation von Daten](https://www.sciencedirect.com/topics/computer-science/misrepresentation) bezieht sich darauf, ob wir Erkenntnisse aus ehrlich berichteten Daten auf eine t√§uschende Weise kommunizieren, um eine gew√ºnschte Erz√§hlung zu unterst√ºtzen.

Fragen, die hier untersucht werden sollten:
* Berichten wir unvollst√§ndige oder ungenaue Daten?
* Visualisieren wir Daten so, dass sie zu irref√ºhrenden Schlussfolgerungen f√ºhren?
* Verwenden wir selektive statistische Techniken, um Ergebnisse zu manipulieren?
* Gibt es alternative Erkl√§rungen, die zu einer anderen Schlussfolgerung f√ºhren k√∂nnten?

#### 2.10 Freie Wahl

Die [Illusion der freien Wahl](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) tritt auf, wenn "Entscheidungsarchitekturen" von Systemen Algorithmen verwenden, um Menschen dazu zu bewegen, ein bevorzugtes Ergebnis zu w√§hlen, w√§hrend sie scheinbar Optionen und Kontrolle haben. Diese [dunklen Muster](https://www.darkpatterns.org/) k√∂nnen sozialen und wirtschaftlichen Schaden f√ºr Nutzer verursachen. Da Nutzerentscheidungen Verhaltensprofile beeinflussen, k√∂nnen diese Aktionen zuk√ºnftige Entscheidungen antreiben, die die Auswirkungen dieser Sch√§den verst√§rken oder verl√§ngern.

Fragen, die hier untersucht werden sollten:
* Hat der Nutzer die Auswirkungen dieser Entscheidung verstanden?
* War sich der Nutzer der (alternativen) Optionen und der Vor- und Nachteile jeder Option bewusst?
* Kann der Nutzer eine automatisierte oder beeinflusste Entscheidung sp√§ter r√ºckg√§ngig machen?

### 3. Fallstudien

Um diese ethischen Herausforderungen in realen Kontexten zu verstehen, hilft es, Fallstudien zu betrachten, die die potenziellen Sch√§den und Konsequenzen f√ºr Einzelpersonen und die Gesellschaft aufzeigen, wenn solche ethischen Verst√∂√üe √ºbersehen werden.

Hier sind einige Beispiele:

| Ethische Herausforderung | Fallstudie | 
|--- |--- |
| **Informierte Zustimmung** | 1972 - [Tuskegee-Syphilis-Studie](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - Afroamerikanische M√§nner, die an der Studie teilnahmen, wurden kostenlose medizinische Versorgung versprochen, _aber von Forschern get√§uscht_, die die Teilnehmer nicht √ºber ihre Diagnose oder die Verf√ºgbarkeit von Behandlung informierten. Viele Teilnehmer starben, und Partner oder Kinder waren betroffen; die Studie dauerte 40 Jahre. | 
| **Datenschutz** | 2007 - Der [Netflix-Datenpreis](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) stellte Forschern _10 Millionen anonymisierte Filmbewertungen von 50.000 Kunden_ zur Verf√ºgung, um Empfehlungsalgorithmen zu verbessern. Forscher konnten jedoch anonymisierte Daten mit pers√∂nlich identifizierbaren Daten in _externen Datens√§tzen_ (z. B. IMDb-Kommentaren) korrelieren und einige Netflix-Abonnenten effektiv "de-anonymisieren". |
| **Sammlungsbias** | 2013 - Die Stadt Boston [entwickelte Street Bump](https://www.boston.gov/transportation/street-bump), eine App, mit der B√ºrger Schlagl√∂cher melden konnten, um der Stadt bessere Stra√üendaten zur Verf√ºgung zu stellen. Allerdings hatten [Menschen in einkommensschw√§cheren Gruppen weniger Zugang zu Autos und Telefonen](https://hbr.org/2013/04/the-hidden-biases-in-big-data), wodurch ihre Stra√üenprobleme in dieser App unsichtbar wurden. Entwickler arbeiteten mit Akademikern zusammen, um _Probleme des gerechten Zugangs und der digitalen Kluft_ f√ºr mehr Fairness zu l√∂sen. |
| **Fairness von Algorithmen** | 2018 - Die MIT-Studie [Gender Shades](http://gendershades.org/overview.html) bewertete die Genauigkeit von KI-Produkten zur Geschlechtsklassifikation und deckte L√ºcken in der Genauigkeit f√ºr Frauen und farbige Personen auf. Eine [Apple Card 2019](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) schien Frauen weniger Kredit zu gew√§hren als M√§nnern. Beide F√§lle illustrierten Probleme des algorithmischen Bias, die zu sozio√∂konomischen Sch√§den f√ºhrten. |
| **Fehlrepr√§sentation von Daten** | 2020 - Das [Georgia Department of Public Health ver√∂ffentlichte COVID-19-Diagramme](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening), die B√ºrger √ºber Trends bei best√§tigten F√§llen mit nicht-chronologischer Reihenfolge auf der x-Achse irref√ºhren sollten. Dies zeigt Fehlrepr√§sentation durch Visualisierungstricks. |
| **Illusion der freien Wahl** | 2020 - Lern-App [ABCmouse zahlte $10M, um eine FTC-Beschwerde beizulegen](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/), bei der Eltern in Abonnements gefangen waren, die sie nicht k√ºndigen konnten. Dies zeigt dunkle Muster in Entscheidungsarchitekturen, bei denen Nutzer zu potenziell sch√§dlichen Entscheidungen gedr√§ngt wurden. |
| **Datenschutz & Nutzerrechte** | 2021 - Facebook [Datenleck](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) enth√ºllte Daten von 530 Millionen Nutzern, was zu einer $5B-Einigung mit der FTC f√ºhrte. Facebook weigerte sich jedoch, die Nutzer √ºber das Leck zu informieren, was die Nutzerrechte in Bezug auf Datentransparenz und Zugang verletzte. |

M√∂chten Sie weitere Fallstudien erkunden? Schauen Sie sich diese Ressourcen an:
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - ethische Dilemmata in verschiedenen Branchen.
* [Data Science Ethics course](https://www.coursera.org/learn/data-science-ethics#syllabus) - wegweisende Fallstudien.
* [Wo Dinge schiefgelaufen sind](https://deon.drivendata.org/examples/) - Deon-Checkliste mit Beispielen.

> üö® Denken Sie an die Fallstudien, die Sie gesehen haben ‚Äì haben Sie √§hnliche ethische Herausforderungen in Ihrem Leben erlebt oder waren davon betroffen? K√∂nnen Sie mindestens eine weitere Fallstudie nennen, die eine der ethischen Herausforderungen in diesem Abschnitt illustriert?

## Angewandte Ethik

Wir haben √ºber ethische Konzepte, Herausforderungen und Fallstudien in realen Kontexten gesprochen. Aber wie beginnen wir, _ethische Prinzipien und Praktiken_ in unseren Projekten anzuwenden? Und wie _operationalisieren_ wir diese Praktiken f√ºr eine bessere Governance? Lassen Sie uns einige L√∂sungen aus der Praxis erkunden:

### 1. Berufliche Kodizes

Berufliche Kodizes bieten eine M√∂glichkeit f√ºr Organisationen, Mitglieder zu "motivieren", ihre ethischen Prinzipien und ihre Mission zu unterst√ºtzen. Kodizes sind _moralische Leitlinien_ f√ºr berufliches Verhalten, die Mitarbeitern oder Mitgliedern helfen, Entscheidungen zu treffen, die mit den Prinzipien ihrer Organisation √ºbereinstimmen. Sie sind nur so gut wie die freiwillige Einhaltung durch die Mitglieder; viele Organisationen bieten jedoch zus√§tzliche Belohnungen und Strafen, um die Einhaltung zu f√∂rdern.

Beispiele:
* [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/) Ethikkodex
* [Data Science Association](http://datascienceassn.org/code-of-conduct.html) Verhaltenskodex (erstellt 2013)
* [ACM Code of Ethics and Professional Conduct](https://www.acm.org/code-of-ethics) (seit 1993)

> üö® Sind Sie Mitglied einer professionellen Ingenieur- oder Datenwissenschaftsorganisation? Erkunden Sie deren Website, um zu sehen, ob sie einen beruflichen Ethikkodex definieren. Was sagt dieser √ºber ihre ethischen Prinzipien aus? Wie motivieren sie Mitglieder, den Kodex zu befolgen?

### 2. Ethik-Checklisten

W√§hrend berufliche Kodizes das erforderliche _ethische Verhalten_ von Praktikern definieren, haben sie [bekannte Einschr√§nkungen](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) bei der Durchsetzung, insbesondere in gro√ü angelegten Projekten. Stattdessen bef√ºrworten viele Datenwissenschaftler [Checklisten](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md), die **Prinzipien mit Praktiken** auf deterministische und umsetzbare Weise verbinden k√∂nnen.

Checklisten wandeln Fragen in "Ja/Nein"-Aufgaben um, die operationalisiert werden k√∂nnen, sodass sie als Teil standardm√§√üiger Produktfreigabeworkflows verfolgt werden k√∂nnen.

Beispiele:
* [Deon](https://deon.drivendata.org/) - eine allgemeine Checkliste f√ºr Datenethik, erstellt aus [Brancheneempfehlungen](https://deon.drivendata.org/#checklist-citations) mit einem Befehlszeilentool f√ºr einfache Integration.
* [Privacy Audit Checklist](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - bietet allgemeine Leitlinien f√ºr Informationshandhabungspraktiken aus rechtlicher und sozialer Perspektive.
* [AI Fairness Checklist](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - erstellt von KI-Praktikern zur Unterst√ºtzung der Integration von Fairness-Checks in KI-Entwicklungszyklen.
* [22 Fragen zur Ethik in Daten und KI](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - ein offeneres Framework, strukturiert f√ºr die erste Untersuchung ethischer Fragen in Design-, Implementierungs- und Organisationskontexten.

### 3. Ethik-Regulierungen

Ethik bedeutet, gemeinsame Werte zu definieren und das Richtige _freiwillig_ zu tun. **Compliance** bedeutet, _das Gesetz zu befolgen_, wenn und wo es definiert ist. **Governance** umfasst allgemein alle M√∂glichkeiten, wie Organisationen ethische Prinzipien durchsetzen und gesetzliche Vorschriften einhalten.

Heute nimmt Governance in Organisationen zwei Formen an. Erstens geht es darum, **ethische KI**-Prinzipien zu definieren und Praktiken zu etablieren, um die Einf√ºhrung in allen KI-bezogenen Projekten der Organisation zu operationalisieren. Zweitens geht es darum, alle staatlich vorgeschriebenen **Datenschutzvorschriften** f√ºr die Regionen, in denen sie t√§tig sind, einzuhalten.

Beispiele f√ºr Datenschutz- und Privatsph√§re-Regulierungen:
* `1974`, [US Privacy Act](https://www.justice.gov/opcl/privacy-act-1974) - regelt die Sammlung, Nutzung und Offenlegung pers√∂nlicher Informationen durch die _Bundesregierung_.
* `1996`, [US Health Insurance Portability & Accountability Act (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - sch√ºtzt pers√∂nliche Gesundheitsdaten.
* `1998`, [US Children's Online Privacy Protection Act (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - sch√ºtzt die Datenprivatsph√§re von Kindern unter 13 Jahren.
* `2018`, [General Data Protection Regulation (GDPR)](https://gdpr-info.eu/) - bietet Nutzerrechte, Datenschutz und Privatsph√§re.
* `2018`, [California Consumer Privacy Act (CCPA)](https://www.oag.ca.gov/privacy/ccpa) gibt Verbrauchern mehr _Rechte_ √ºber ihre (pers√∂nlichen) Daten.
* `2021`, Chinas [Gesetz zum Schutz pers√∂nlicher Informationen](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) wurde gerade verabschiedet und schafft eine der weltweit st√§rksten Online-Datenschutzvorschriften.

> üö® Die Europ√§ische Union hat die GDPR (General Data Protection Regulation) definiert, die heute eine der einflussreichsten Datenschutzvorschriften ist. Wussten Sie, dass sie auch [8 Nutzerrechte](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) definiert, um die digitale Privatsph√§re und pers√∂nlichen Daten der B√ºrger zu sch√ºtzen? Lernen Sie, was diese sind und warum sie wichtig sind.

### 4. Ethikkultur

Beachten Sie, dass es eine immaterielle L√ºcke zwischen _Compliance_ (genug tun, um "den Buchstaben des Gesetzes" zu erf√ºllen) und der Bew√§ltigung [systemischer Probleme](https://www.coursera.org/learn/data-science-ethics/home/week/4) (wie Versteinerung, Informationsasymmetrie und Verteilungsungerechtigkeit) gibt, die die Waffenf√§higkeit von KI beschleunigen k√∂nnen.

Letzteres erfordert [kollaborative Ans√§tze zur Definition von Ethikkulturen](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f), die emotionale Verbindungen und konsistente gemeinsame Werte _√ºber Organisationen hinweg_ in der Branche schaffen. Dies erfordert mehr [formalisierte Datenethikkulturen](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) in Organisationen ‚Äì sodass _jeder_ [die Andon-Schnur ziehen](https://en.wikipedia.org/wiki/Andon_(manufacturing)) kann (um ethische Bedenken fr√ºhzeitig im Prozess zu √§u√üern) und _ethische Bewertungen_ (z. B. bei der Einstellung) zu einem Kernkriterium der Teamzusammenstellung in KI-Projekten macht.

---
## [Quiz nach der Vorlesung](https://ff-quizzes.netlify.app/en/ds/quiz/3) üéØ
## √úberpr√ºfung & Selbststudium

Kurse und B√ºcher helfen, die grundlegenden ethischen Konzepte und Herausforderungen zu verstehen, w√§hrend Fallstudien und Tools bei der Anwendung ethischer Praktiken in realen Kontexten helfen. Hier sind einige Ressourcen, um zu beginnen.
* [Maschinelles Lernen f√ºr Anf√§nger](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - Lektion √ºber Fairness von Microsoft.  
* [Grunds√§tze der verantwortungsvollen KI](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - kostenloser Lernpfad von Microsoft Learn.  
* [Ethik und Data Science](https://resources.oreilly.com/examples/0636920203964) - O'Reilly E-Book (M. Loukides, H. Mason et. al).  
* [Ethik in der Datenwissenschaft](https://www.coursera.org/learn/data-science-ethics#syllabus) - Online-Kurs der Universit√§t von Michigan.  
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - Fallstudien der Universit√§t von Texas.  

# Aufgabe  

[Schreiben Sie eine Fallstudie zur Datenethik](assignment.md)  

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.