<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "58860ce9a4b8a564003d2752f7c72851",
  "translation_date": "2025-10-03T16:39:37+00:00",
  "source_file": "1-Introduction/02-ethics/README.md",
  "language_code": "no"
}
-->
# Introduksjon til Dataetikk

|![ Sketchnote av [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| Data Science Ethics - _Sketchnote av [@nitya](https://twitter.com/nitya)_ |

---

Vi er alle databorgere som lever i en verden preget av data.

Markedsanalyser viser at innen 2022 vil √©n av tre store organisasjoner kj√∏pe og selge data gjennom online [markedsplasser og utvekslinger](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/). Som **apputviklere** vil vi oppleve at det blir enklere og billigere √• integrere datadrevne innsikter og algoritmestyrt automatisering i daglige brukeropplevelser. Men ettersom AI blir mer utbredt, m√• vi ogs√• forst√• de potensielle skadene som kan oppst√• ved [v√•penisering](https://www.youtube.com/watch?v=TQHs8SA1qpk) av slike algoritmer i stor skala.

Trender antyder at innen 2025 vil vi generere og konsumere over [180 zettabyte](https://www.statista.com/statistics/871513/worldwide-data-created/) med data. For **dataforskere** gir denne eksplosjonen av informasjon enest√•ende tilgang til personlig og atferdsrelatert data. Med dette kommer makten til √• bygge detaljerte brukerprofiler og subtilt p√•virke beslutningstaking‚Äîofte p√• m√•ter som skaper en [illusjon av fri vilje](https://www.datasciencecentral.com/the-pareto-set-and-the-paradox-of-choice/). Selv om dette kan brukes til √• dytte brukere mot √∏nskede utfall, reiser det ogs√• kritiske sp√∏rsm√•l om dataprivacy, autonomi og de etiske grensene for algoritmisk p√•virkning.

Dataetikk er n√• _n√∏dvendige retningslinjer_ for dataforskning og ingeni√∏rarbeid, som hjelper oss med √• minimere potensielle skader og utilsiktede konsekvenser av v√•re datadrevne handlinger. [Gartner Hype Cycle for AI](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) identifiserer relevante trender innen digital etikk, ansvarlig AI og AI-styring som n√∏kkeldrivere for st√∏rre megatrender rundt _demokratisering_ og _industrialisering_ av AI.

![Gartner's Hype Cycle for AI - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

I denne leksjonen skal vi utforske det fascinerende omr√•det dataetikk - fra grunnleggende konsepter og utfordringer til casestudier og anvendte AI-konsepter som styring - som hjelper med √• etablere en etisk kultur i team og organisasjoner som jobber med data og AI.

## [Quiz f√∏r forelesning](https://ff-quizzes.netlify.app/en/ds/quiz/2) üéØ

## Grunnleggende definisjoner

La oss starte med √• forst√• grunnleggende terminologi.

Ordet "etikk" kommer fra det [greske ordet "ethikos"](https://en.wikipedia.org/wiki/Ethics) (og dets rot "ethos") som betyr _karakter eller moralsk natur_. 

**Etikk** handler om de delte verdiene og moralske prinsippene som styrer v√•r oppf√∏rsel i samfunnet. Etikk er ikke basert p√• lover, men p√• bredt aksepterte normer for hva som er "riktig vs. galt". Imidlertid kan etiske hensyn p√•virke initiativer for selskapsstyring og myndighetsreguleringer som skaper flere insentiver for samsvar.

**Dataetikk** er en [ny gren av etikk](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1) som "studerer og evaluerer moralske problemer knyttet til _data, algoritmer og tilh√∏rende praksis_". Her fokuserer **"data"** p√• handlinger relatert til generering, registrering, kuratering, behandling, spredning, deling og bruk, **"algoritmer"** fokuserer p√• AI, agenter, maskinl√¶ring og roboter, og **"praksis"** fokuserer p√• temaer som ansvarlig innovasjon, programmering, hacking og etiske koder.

**Anvendt etikk** er den [praktiske anvendelsen av moralske hensyn](https://en.wikipedia.org/wiki/Applied_ethics). Det er prosessen med aktivt √• unders√∏ke etiske sp√∏rsm√•l i konteksten av _virkelige handlinger, produkter og prosesser_, og ta korrigerende tiltak for √• sikre at disse forblir i tr√•d med v√•re definerte etiske verdier.

**Etisk kultur** handler om [_operasjonalisering_ av anvendt etikk](https://hbr.org/2019/05/how-to-design-an-ethical-organization) for √• sikre at v√•re etiske prinsipper og praksiser blir tatt i bruk p√• en konsekvent og skalerbar m√•te i hele organisasjonen. Vellykkede etiske kulturer definerer organisasjonsomfattende etiske prinsipper, gir meningsfulle insentiver for samsvar og forsterker etiske normer ved √• oppmuntre og forsterke √∏nsket atferd p√• alle niv√•er i organisasjonen.

## Etiske konsepter

I denne delen skal vi diskutere konsepter som **delte verdier** (prinsipper) og **etiske utfordringer** (problemer) for dataetikk - og utforske **casestudier** som hjelper deg med √• forst√• disse konseptene i virkelige kontekster.

### 1. Etiske prinsipper

Hver dataetikkstrategi begynner med √• definere _etiske prinsipper_ - de "delte verdiene" som beskriver akseptabel oppf√∏rsel og veileder samsvarende handlinger i v√•re data- og AI-prosjekter. Du kan definere disse p√• individ- eller teamniv√•. Imidlertid skisserer de fleste store organisasjoner disse i en _etisk AI_-misjonserkl√¶ring eller rammeverk som er definert p√• selskapsniv√• og h√•ndhevet konsekvent p√• tvers av alle team.

**Eksempel:** Microsofts [Responsible AI](https://www.microsoft.com/en-us/ai/responsible-ai)-misjonserkl√¶ring lyder: _"Vi er forpliktet til √• fremme AI drevet av etiske prinsipper som setter mennesker f√∏rst"_ - og identifiserer 6 etiske prinsipper i rammeverket nedenfor:

![Responsible AI hos Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

La oss kort utforske disse prinsippene. _√Öpenhet_ og _ansvarlighet_ er grunnleggende verdier som de andre prinsippene bygger p√• - s√• la oss begynne der:

* [**Ansvarlighet**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) gj√∏r ut√∏vere _ansvarlige_ for sine data- og AI-operasjoner og samsvar med disse etiske prinsippene.
* [**√Öpenhet**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) sikrer at data- og AI-handlinger er _forst√•elige_ (tolkbare) for brukere, og forklarer hva og hvorfor bak beslutninger.
* [**Rettferdighet**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) fokuserer p√• √• sikre at AI behandler _alle mennesker_ rettferdig, og adresserer eventuelle systemiske eller implisitte sosio-tekniske skjevheter i data og systemer.
* [**P√•litelighet og sikkerhet**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) sikrer at AI oppf√∏rer seg _konsekvent_ med definerte verdier, og minimerer potensielle skader eller utilsiktede konsekvenser.
* [**Personvern og sikkerhet**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) handler om √• forst√• dataenes opprinnelse og gi _datapersonvern og relaterte beskyttelser_ til brukere.
* [**Inkludering**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) handler om √• designe AI-l√∏sninger med intensjon, og tilpasse dem for √• m√∏te et _bredt spekter av menneskelige behov_ og evner.

> üö® Tenk p√• hva din dataetikk-misjonerkl√¶ring kunne v√¶rt. Utforsk etiske AI-rammeverk fra andre organisasjoner - her er eksempler fra [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles) og [Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). Hvilke delte verdier har de til felles? Hvordan relaterer disse prinsippene seg til AI-produktet eller industrien de opererer i?

### 2. Etiske utfordringer

N√•r vi har definert etiske prinsipper, er neste steg √• evaluere v√•re data- og AI-handlinger for √• se om de samsvarer med disse delte verdiene. Tenk p√• handlingene dine i to kategorier: _datainnsamling_ og _algoritmedesign_. 

Ved datainnsamling vil handlingene sannsynligvis involvere **personlige data** eller personlig identifiserbar informasjon (PII) for identifiserbare levende individer. Dette inkluderer [mangfoldige elementer av ikke-personlige data](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en) som _samlet sett_ identifiserer en person. Etiske utfordringer kan relatere seg til _datapersonvern_, _dataeierskap_ og relaterte temaer som _informert samtykke_ og _immaterielle rettigheter_ for brukere.

Ved algoritmedesign vil handlingene involvere innsamling og kuratering av **datasett**, og deretter bruke dem til √• trene og distribuere **datamodeller** som forutsier utfall eller automatiserer beslutninger i virkelige kontekster. Etiske utfordringer kan oppst√• fra _datasettbias_, _datakvalitetsproblemer_, _urettferdighet_ og _feilrepresentasjon_ i algoritmer - inkludert noen problemer som er systemiske av natur.

I begge tilfeller fremhever etiske utfordringer omr√•der der v√•re handlinger kan komme i konflikt med v√•re delte verdier. For √• oppdage, redusere, minimere eller eliminere disse bekymringene - m√• vi stille moralske "ja/nei"-sp√∏rsm√•l knyttet til v√•re handlinger, og deretter ta korrigerende tiltak etter behov. La oss se p√• noen etiske utfordringer og de moralske sp√∏rsm√•lene de reiser:

#### 2.1 Dataeierskap

Datainnsamling involverer ofte personlige data som kan identifisere datasubjektene. [Dataeierskap](https://permission.io/blog/data-ownership) handler om _kontroll_ og [_brukerrettigheter_](https://permission.io/blog/data-ownership) knyttet til opprettelse, behandling og spredning av data. 

De moralske sp√∏rsm√•lene vi m√• stille er: 
 * Hvem eier dataene? (bruker eller organisasjon)
 * Hvilke rettigheter har datasubjektene? (f.eks. tilgang, sletting, portabilitet)
 * Hvilke rettigheter har organisasjoner? (f.eks. rette opp ondsinnede brukeranmeldelser)

#### 2.2 Informert samtykke

[Informert samtykke](https://legaldictionary.net/informed-consent/) definerer handlingen der brukere samtykker til en handling (som datainnsamling) med en _full forst√•else_ av relevante fakta, inkludert form√•l, potensielle risikoer og alternativer. 

Sp√∏rsm√•l √• utforske her er:
 * Ga brukeren (datasubjektet) tillatelse til datainnsamling og bruk?
 * Forsto brukeren form√•let med at dataene ble samlet inn?
 * Forsto brukeren de potensielle risikoene ved deres deltakelse?

#### 2.3 Immaterielle rettigheter

[Immaterielle rettigheter](https://en.wikipedia.org/wiki/Intellectual_property) refererer til immaterielle skapelser som f√∏lge av menneskelig initiativ, som kan _ha √∏konomisk verdi_ for individer eller bedrifter. 

Sp√∏rsm√•l √• utforske her er:
 * Hadde de innsamlede dataene √∏konomisk verdi for en bruker eller bedrift?
 * Har **brukeren** immaterielle rettigheter her?
 * Har **organisasjonen** immaterielle rettigheter her?
 * Hvis disse rettighetene eksisterer, hvordan beskytter vi dem?

#### 2.4 Datapersonvern

[Datapersonvern](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) eller informasjonsvern refererer til bevaring av brukerens personvern og beskyttelse av brukerens identitet med hensyn til personlig identifiserbar informasjon. 

Sp√∏rsm√•l √• utforske her er:
 * Er brukernes (personlige) data sikret mot hacking og lekkasjer?
 * Er brukernes data kun tilgjengelig for autoriserte brukere og kontekster?
 * Er brukernes anonymitet bevart n√•r data deles eller spres?
 * Kan en bruker bli avidentifisert fra anonymiserte datasett?

#### 2.5 Rett til √• bli glemt

[Rett til √• bli glemt](https://en.wikipedia.org/wiki/Right_to_be_forgotten) eller [rett til sletting](https://www.gdpreu.org/right-to-be-forgotten/) gir ekstra beskyttelse av personlige data til brukere. Spesielt gir det brukere rett til √• be om sletting eller fjerning av personlige data fra Internett-s√∏k og andre steder, _under spesifikke omstendigheter_ - slik at de kan f√• en ny start online uten at tidligere handlinger blir holdt mot dem.

Sp√∏rsm√•l √• utforske her er:
 * Tillater systemet datasubjekter √• be om sletting?
 * B√∏r tilbaketrekking av brukersamtykke utl√∏se automatisk sletting?
 * Ble data samlet inn uten samtykke eller p√• ulovlig vis?
 * Er vi i samsvar med myndighetsreguleringer for dataprivacy?

#### 2.6 Datasettbias

Datasett eller [innsamlingsbias](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) handler om √• velge et _ikke-representativt_ datasett for algoritmeutvikling, noe som kan skape potensielle urettferdigheter i resultatene for ulike grupper. Typer bias inkluderer utvalgsbias, frivillighetsbias og instrumentbias. 

Sp√∏rsm√•l √• utforske her er:
 * Rekrutterte vi et representativt sett med datasubjekter?
 * Testet vi v√•rt innsamlede eller kuraterte datasett for ulike bias?
 * Kan vi redusere eller fjerne oppdagede bias?

#### 2.7 Datakvalitet

[Datakvalitet](https://lakefs.io/data-quality-testing/) ser p√• gyldigheten av det kuraterte datasettet som brukes til √• utvikle v√•re algoritmer, og sjekker om funksjoner og poster oppfyller kravene til niv√•et av n√∏yaktighet og konsistens som trengs for v√•rt AI-form√•l.

Sp√∏rsm√•l √• utforske her er:
 * Fanget vi gyldige _funksjoner_ for v√•r brukssak?
 * Ble data fanget _konsekvent_ p√• tvers av ulike datakilder?
 * Er datasettet _komplett_ for ulike forhold eller scenarier?
* Blir informasjon fanget opp _n√∏yaktig_ og reflekterer den virkeligheten?

#### 2.8 Algoritmisk rettferdighet

[Algoritmisk rettferdighet](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) handler om √• unders√∏ke om algoritmedesign systematisk diskriminerer spesifikke undergrupper av datasubjekter, noe som kan f√∏re til [potensielle skader](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) i _fordeling_ (der ressurser nektes eller holdes tilbake fra den gruppen) og _tjenestekvalitet_ (der AI ikke er like n√∏yaktig for noen undergrupper som for andre).

Sp√∏rsm√•l √• utforske her:
* Evaluerte vi modellens n√∏yaktighet for ulike undergrupper og forhold?
* Unders√∏kte vi systemet for potensielle skader (f.eks. stereotypier)?
* Kan vi revidere data eller trene opp modeller p√• nytt for √• redusere identifiserte skader?

Utforsk ressurser som [AI Fairness-sjekklister](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) for √• l√¶re mer.

#### 2.9 Feilrepresentasjon

[Feilrepresentasjon av data](https://www.sciencedirect.com/topics/computer-science/misrepresentation) handler om √• sp√∏rre om vi kommuniserer innsikter fra √¶rlig rapporterte data p√• en villedende m√•te for √• st√∏tte en √∏nsket fortelling.

Sp√∏rsm√•l √• utforske her:
* Rapporterer vi ufullstendige eller un√∏yaktige data?
* Visualiserer vi data p√• en m√•te som gir villedende konklusjoner?
* Bruker vi selektive statistiske teknikker for √• manipulere resultater?
* Finnes det alternative forklaringer som kan gi en annen konklusjon?

#### 2.10 Fri vilje
[Illusjonen av fri vilje](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) oppst√•r n√•r systemets "valgarkitekturer" bruker beslutningsalgoritmer for √• p√•virke folk til √• ta et foretrukket utfall, samtidig som det ser ut som de har alternativer og kontroll. Disse [m√∏rke m√∏nstrene](https://www.darkpatterns.org/) kan for√•rsake sosial og √∏konomisk skade for brukere. Fordi brukerbeslutninger p√•virker atferdsprofiler, kan disse handlingene potensielt drive fremtidige valg som forsterker eller utvider virkningen av disse skadene.

Sp√∏rsm√•l √• utforske her:
* Forsto brukeren konsekvensene av √• ta det valget?
* Var brukeren klar over (alternative) valg og fordeler og ulemper ved hvert?
* Kan brukeren reversere et automatisert eller p√•virket valg senere?

### 3. Case-studier

For √• sette disse etiske utfordringene i en virkelighetsn√¶r kontekst, kan det v√¶re nyttig √• se p√• case-studier som fremhever potensielle skader og konsekvenser for enkeltpersoner og samfunnet n√•r slike etiske brudd blir oversett.

Her er noen eksempler:

| Etisk utfordring | Case-studie | 
|--- |--- |
| **Informert samtykke** | 1972 - [Tuskegee Syfilis-studien](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - Afroamerikanske menn som deltok i studien ble lovet gratis medisinsk behandling _men ble lurt_ av forskere som unnlot √• informere deltakerne om diagnosen eller tilgjengelig behandling. Mange d√∏de, og partnere eller barn ble p√•virket; studien varte i 40 √•r. | 
| **Datapersonvern** | 2007 - [Netflix data-prisen](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) ga forskere _10M anonymiserte filmrangeringer fra 50K kunder_ for √• forbedre anbefalingsalgoritmer. Imidlertid klarte forskere √• korrelere anonymiserte data med personlig identifiserbare data i _eksterne datasett_ (f.eks. IMDb-kommentarer) - effektivt "de-anonymiserte" noen Netflix-abonnenter.|
| **Innsamlingsskjevhet** | 2013 - Byen Boston [utviklet Street Bump](https://www.boston.gov/transportation/street-bump), en app som lot innbyggere rapportere hull i veien, og ga byen bedre data for √• finne og fikse problemer. Imidlertid hadde [folk i lavinntektsgrupper mindre tilgang til biler og telefoner](https://hbr.org/2013/04/the-hidden-biases-in-big-data), noe som gjorde deres veiproblemer usynlige i denne appen. Utviklerne samarbeidet med akademikere for √• adressere _rettferdig tilgang og digitale skiller_. |
| **Algoritmisk rettferdighet** | 2018 - MIT [Gender Shades-studien](http://gendershades.org/overview.html) evaluerte n√∏yaktigheten til AI-produkter for kj√∏nnsidentifikasjon, og avdekket mangler i n√∏yaktighet for kvinner og personer med m√∏rk hud. Et [2019 Apple Card](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) syntes √• tilby mindre kreditt til kvinner enn menn. Begge eksemplene illustrerte problemer med algoritmisk skjevhet som f√∏rte til sosio√∏konomiske skader.|
| **Feilrepresentasjon av data** | 2020 - [Georgia Department of Public Health publiserte COVID-19-diagrammer](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening) som syntes √• villede innbyggerne om trender i bekreftede tilfeller med ikke-kronologisk rekkef√∏lge p√• x-aksen. Dette illustrerer feilrepresentasjon gjennom visualiseringstriks. |
| **Illusjon av fri vilje** | 2020 - L√¶ringsappen [ABCmouse betalte $10M for √• l√∏se en FTC-klage](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/) der foreldre ble fanget i abonnementer de ikke kunne kansellere. Dette illustrerer m√∏rke m√∏nstre i valgarkitekturer, der brukere ble p√•virket til potensielt skadelige valg. |
| **Datapersonvern og brukerrettigheter** | 2021 - Facebook [Data-lekkasje](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) eksponerte data fra 530M brukere, noe som resulterte i et $5B forlik med FTC. Facebook nektet imidlertid √• varsle brukere om lekkasjen, og br√∏t brukerrettigheter rundt datatransparens og tilgang. |

Vil du utforske flere case-studier? Sjekk ut disse ressursene:
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - etiske dilemmaer p√• tvers av ulike bransjer. 
* [Data Science Ethics-kurs](https://www.coursera.org/learn/data-science-ethics#syllabus) - utforsker viktige case-studier.
* [Hvor ting har g√•tt galt](https://deon.drivendata.org/examples/) - deon-sjekkliste med eksempler.

> üö® Tenk p√• case-studiene du har sett - har du opplevd, eller blitt p√•virket av, en lignende etisk utfordring i ditt liv? Kan du komme p√• minst √©n annen case-studie som illustrerer en av de etiske utfordringene vi har diskutert i denne delen?

## Anvendt etikk

Vi har snakket om etiske konsepter, utfordringer og case-studier i virkelighetsn√¶re kontekster. Men hvordan kommer vi i gang med √• _anvende_ etiske prinsipper og praksiser i prosjektene v√•re? Og hvordan _operasjonaliserer_ vi disse praksisene for bedre styring? La oss utforske noen virkelighetsn√¶re l√∏sninger:

### 1. Profesjonelle koder

Profesjonelle koder tilbyr √©n mulighet for organisasjoner til √• "incentivere" medlemmer til √• st√∏tte deres etiske prinsipper og misjonserkl√¶ring. Kodene er _moralske retningslinjer_ for profesjonell oppf√∏rsel, som hjelper ansatte eller medlemmer med √• ta beslutninger som samsvarer med organisasjonens prinsipper. De er kun s√• gode som den frivillige etterlevelsen fra medlemmene; imidlertid tilbyr mange organisasjoner ekstra bel√∏nninger og straffer for √• motivere etterlevelse.

Eksempler inkluderer:

* [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/) Etisk kode
* [Data Science Association](http://datascienceassn.org/code-of-conduct.html) Etisk kode (opprettet 2013)
* [ACM Code of Ethics and Professional Conduct](https://www.acm.org/code-of-ethics) (siden 1993)

> üö® Tilh√∏rer du en profesjonell ingeni√∏r- eller dataorganisasjon? Utforsk nettstedet deres for √• se om de definerer en profesjonell etisk kode. Hva sier dette om deres etiske prinsipper? Hvordan "incentiverer" de medlemmene til √• f√∏lge koden?

### 2. Etiske sjekklister

Mens profesjonelle koder definerer n√∏dvendig _etisk oppf√∏rsel_ fra ut√∏vere, har de [kjente begrensninger](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) i h√•ndheving, spesielt i storskala prosjekter. I stedet anbefaler mange dataeksperter [sjekklister](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md), som kan **koble prinsipper til praksis** p√• mer deterministiske og handlingsrettede m√•ter.

Sjekklister konverterer sp√∏rsm√•l til "ja/nei"-oppgaver som kan operasjonaliseres, slik at de kan spores som en del av standard produktutgivelsesarbeidsflyter.

Eksempler inkluderer:
* [Deon](https://deon.drivendata.org/) - en generell sjekkliste for dataetikk opprettet fra [bransjeanbefalinger](https://deon.drivendata.org/#checklist-citations) med et kommandolinjeverkt√∏y for enkel integrasjon.
* [Privacy Audit Checklist](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - gir generell veiledning for informasjonsbehandlingspraksis fra juridiske og sosiale eksponeringsperspektiver.
* [AI Fairness Checklist](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - opprettet av AI-ut√∏vere for √• st√∏tte adopsjon og integrering av rettferdighetssjekker i AI-utviklingssykluser.
* [22 sp√∏rsm√•l for etikk i data og AI](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - en mer √•pen rammeverk, strukturert for innledende utforskning av etiske sp√∏rsm√•l i design, implementering og organisatoriske kontekster.

### 3. Etiske reguleringer

Etikk handler om √• definere felles verdier og gj√∏re det rette _frivillig_. **Etterlevelse** handler om _√• f√∏lge loven_ der den er definert. **Styring** dekker bredt alle m√•ter organisasjoner opererer for √• h√•ndheve etiske prinsipper og overholde etablerte lover.

I dag tar styring to former innen organisasjoner. For det f√∏rste handler det om √• definere **etiske AI**-prinsipper og etablere praksiser for √• operasjonalisere adopsjon p√• tvers av alle AI-relaterte prosjekter i organisasjonen. For det andre handler det om √• overholde alle myndighetsmandaterte **databeskyttelsesreguleringer** for regionene de opererer i.

Eksempler p√• databeskyttelse og personvernreguleringer:

* `1974`, [US Privacy Act](https://www.justice.gov/opcl/privacy-act-1974) - regulerer _f√∏deral regjering_ sin innsamling, bruk og offentliggj√∏ring av personlig informasjon.
* `1996`, [US Health Insurance Portability & Accountability Act (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - beskytter personlig helsedata.
* `1998`, [US Children's Online Privacy Protection Act (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - beskytter dataprivacy for barn under 13 √•r.
* `2018`, [General Data Protection Regulation (GDPR)](https://gdpr-info.eu/) - gir brukerrettigheter, databeskyttelse og personvern.
* `2018`, [California Consumer Privacy Act (CCPA)](https://www.oag.ca.gov/privacy/ccpa) gir forbrukere flere _rettigheter_ over deres (personlige) data.
* `2021`, Kinas [Personopplysningsbeskyttelseslov](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) ble nettopp vedtatt, og skaper en av de sterkeste online dataprivacy-reguleringene i verden.

> üö® Den europeiske unionens GDPR (General Data Protection Regulation) forblir en av de mest innflytelsesrike dataprivacy-reguleringene i dag. Visste du at den ogs√• definerer [8 brukerrettigheter](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) for √• beskytte borgernes digitale personvern og personlige data? L√¶r om hva disse er, og hvorfor de betyr noe.

### 4. Etisk kultur

Merk at det fortsatt er et immaterielt gap mellom _etterlevelse_ (√• gj√∏re nok for √• oppfylle "lovens bokstav") og √• adressere [systemiske problemer](https://www.coursera.org/learn/data-science-ethics/home/week/4) (som ossifikasjon, informasjonsasymmetri og distribusjonsurettferdighet) som kan akselerere v√•peniseringen av AI.

Det sistnevnte krever [samarbeidende tiln√¶rminger for √• definere etiske kulturer](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f) som bygger emosjonelle forbindelser og konsistente felles verdier _p√• tvers av organisasjoner_ i bransjen. Dette krever mer [formaliserte dataetiske kulturer](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) i organisasjoner - som lar _hvem som helst_ [trekke Andon-snoren](https://en.wikipedia.org/wiki/Andon_(manufacturing)) (for √• reise etiske bekymringer tidlig i prosessen) og gj√∏re _etiske vurderinger_ (f.eks. ved ansettelser) til et kjernekrav for teamdannelse i AI-prosjekter.

---
## [Quiz etter forelesning](https://ff-quizzes.netlify.app/en/ds/quiz/3) üéØ
## Gjennomgang og selvstudium

Kurs og b√∏ker hjelper med √• forst√• kjerneetiske konsepter og utfordringer, mens case-studier og verkt√∏y hjelper med anvendt etikk i virkelighetsn√¶re kontekster. Her er noen ressurser for √• komme i gang.
* [Maskinl√¶ring for nybegynnere](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - leksjon om rettferdighet, fra Microsoft.  
* [Prinsipper for ansvarlig AI](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - gratis l√¶ringssti fra Microsoft Learn.  
* [Etikk og datavitenskap](https://resources.oreilly.com/examples/0636920203964) - O'Reilly EBook (M. Loukides, H. Mason m.fl.)  
* [Etikk innen datavitenskap](https://www.coursera.org/learn/data-science-ethics#syllabus) - nettkurs fra University of Michigan.  
* [Ethics Unwrapped](https://ethicsunwrapped.utexas.edu/case-studies) - casestudier fra University of Texas.  

# Oppgave  

[Skriv en casestudie om dataetikk](assignment.md)  

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi tilstreber n√∏yaktighet, v√¶r oppmerksom p√• at automatiserte oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.